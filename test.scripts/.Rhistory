set.seed(19874)
n <- 10    # Number of observations
p <- 500     # Number of predictors included in model
real_p <- 150  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
nrow(x)
# add colnames
rownames(x) <- sprintf("sample%s", seq(1:nrow(x)))
# vector names
y <- setNames(y, sprintf("sample%s", seq(1:length(y))))
# --------------------------------------------
# Remove NA values
y   <- y[!is.na(y)]
x   <- na.omit(x)
ol  <- intersect(rownames(x), names(y))
y   <- y[ol]
x   <- x[ol, , drop=F]
# create a list of length k, containing the test-set indices
folds <- Folds(y, k=10)
# glmt.net
results <- lapply(folds, function(fold, y, x, parallel=T) {
return(glmnet.wrapper(y=y[-fold],
x=x[-fold, , drop=F],
newx=x[fold,  , drop=F],
newy=y[fold]))
}, y=y, x=x)
# plot
results[[10]]$pred.cor
results[[10]]$pred.cor
results[[10]]$pred
results[[10]]$mse
results[[10]]$coef
results[[10]]
results[[10]]$best.model
plot(results[[10]]$best.model)
for (i in 0:length(folds)) {
plot(results[[i]]$best.model)
}
results[[1]]
Folds <- function(y, k=10) {
n <- length(y)
if (n == 0)
stop('response length is zero')
uniqY <- unique(y)
if (!is.factor(y) && length(y) / length(uniqY) >= k) {
# Intepret the integer-valued y as class labels. Stratify if the number of class labels is <= 5.
y <- factor(y)
} else if (is.numeric(y)) {
# 5-stratum Stratified sampling
if (n >= 5 * k) {
breaks <- unique(quantile(y, probs=seq(0, 1, length.out=5)))
y <- as.integer(cut(y, breaks, include.lowest=TRUE))
} else
y <- rep(1, length(y))
}
sampList <- tapply(seq_along(y), y, sFolds, k=k, simplify=FALSE)
list0 <- list()
length(list0) <- k
samp <- Reduce(function(list1, list2) {
mapply(c, list1, list2, SIMPLIFY=FALSE)
}, sampList, list0)
return(samp)
}
sFolds <- function(yy, k=10) {
if (length(yy) > 1)
allSamp <- sample(yy)
else
allSamp <- yy
n <- length(yy)
nEach <- n %/% k
samp <- list()
length(samp) <- k
for (i in seq_along(samp)) {
if (nEach > 0)
samp[[i]] <- allSamp[1:nEach + (i - 1) * nEach]
else
samp[[i]] <- numeric(0)
}
restSamp <- allSamp[seq(nEach * k + 1, length(allSamp), length.out=length(allSamp) - nEach * k)]
restInd <- sample(k, length(restSamp))
for (i in seq_along(restInd)) {
sampInd <- restInd[i]
samp[[sampInd]] <- c(samp[[sampInd]], restSamp[i])
}
return(samp)
}
# libraries
library(MASS)  # Package needed to generate correlated precictors
library(glmnet)  # Package to fit ridge/lasso/elastic net models
source("elastic.net.R")
# Generate data
set.seed(19874)
n <- 10    # Number of observations
p <- 500     # Number of predictors included in model
real_p <- 150  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
nrow(x)
# add colnames
rownames(x) <- sprintf("sample%s", seq(1:nrow(x)))
# vector names
y <- setNames(y, sprintf("sample%s", seq(1:length(y))))
# --------------------------------------------
# Remove NA values
y   <- y[!is.na(y)]
x   <- na.omit(x)
ol  <- intersect(rownames(x), names(y))
y   <- y[ol]
x   <- x[ol, , drop=F]
# create a list of length k, containing the test-set indices
folds <- Folds(y, k=10)
# glmt.net
results <- lapply(folds, function(fold, y, x, parallel=T) {
return(glmnet.wrapper(y=y[-fold],
x=x[-fold, , drop=F],
newx=x[fold,  , drop=F],
newy=y[fold]))
}, y=y, x=x)
results[[1]]
# plot
for (i in 1:length(folds)) {
plot(results[[i]]$best.model)
}
results[[1]]
results[[1]]$pred
names(results[[1]]$pred)
for (i in 1:length(folds)) {
plot(results[[i]]$best.model, main=names(results[[i]]$pred))
}
results[[1]]
plot(results[[1]]$coef ~ results[[1]]$best.model$lambda)
fit.lasso.cv <- cv.glmnet(x.train, y.train, type.measure="mse", alpha=1,
family="gaussian")
# libraries
library(MASS)  # Package needed to generate correlated precictors
library(glmnet)  # Package to fit ridge/lasso/elastic net models
source("elastic.net.R")
# Generate data
set.seed(19874)
n <- 10    # Number of observations
p <- 500     # Number of predictors included in model
real_p <- 150  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
nrow(x)
# add colnames
rownames(x) <- sprintf("sample%s", seq(1:nrow(x)))
# vector names
y <- setNames(y, sprintf("sample%s", seq(1:length(y))))
# --------------------------------------------
# Remove NA values
y   <- y[!is.na(y)]
x   <- na.omit(x)
ol  <- intersect(rownames(x), names(y))
y   <- y[ol]
x   <- x[ol, , drop=F]
fit.lasso.cv <- cv.glmnet(x.train, y.train, type.measure="mse", alpha=1,
family="gaussian")
Folds <- function(y, k=10) {
n <- length(y)
if (n == 0)
stop('response length is zero')
uniqY <- unique(y)
if (!is.factor(y) && length(y) / length(uniqY) >= k) {
# Intepret the integer-valued y as class labels. Stratify if the number of class labels is <= 5.
y <- factor(y)
} else if (is.numeric(y)) {
# 5-stratum Stratified sampling
if (n >= 5 * k) {
breaks <- unique(quantile(y, probs=seq(0, 1, length.out=5)))
y <- as.integer(cut(y, breaks, include.lowest=TRUE))
} else
y <- rep(1, length(y))
}
sampList <- tapply(seq_along(y), y, sFolds, k=k, simplify=FALSE)
list0 <- list()
length(list0) <- k
samp <- Reduce(function(list1, list2) {
mapply(c, list1, list2, SIMPLIFY=FALSE)
}, sampList, list0)
return(samp)
}
sFolds <- function(yy, k=10) {
if (length(yy) > 1)
allSamp <- sample(yy)
else
allSamp <- yy
n <- length(yy)
nEach <- n %/% k
samp <- list()
length(samp) <- k
for (i in seq_along(samp)) {
if (nEach > 0)
samp[[i]] <- allSamp[1:nEach + (i - 1) * nEach]
else
samp[[i]] <- numeric(0)
}
restSamp <- allSamp[seq(nEach * k + 1, length(allSamp), length.out=length(allSamp) - nEach * k)]
restInd <- sample(k, length(restSamp))
for (i in seq_along(restInd)) {
sampInd <- restInd[i]
samp[[sampInd]] <- c(samp[[sampInd]], restSamp[i])
}
return(samp)
}
# libraries
library(MASS)  # Package needed to generate correlated precictors
library(glmnet)  # Package to fit ridge/lasso/elastic net models
source("elastic.net.R")
# Generate data
set.seed(19874)
n <- 10    # Number of observations
p <- 500     # Number of predictors included in model
real_p <- 150  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
nrow(x)
# add colnames
rownames(x) <- sprintf("sample%s", seq(1:nrow(x)))
# vector names
y <- setNames(y, sprintf("sample%s", seq(1:length(y))))
# --------------------------------------------
# Remove NA values
y   <- y[!is.na(y)]
x   <- na.omit(x)
ol  <- intersect(rownames(x), names(y))
y   <- y[ol]
x   <- x[ol, , drop=F]
# create a list of length k, containing the test-set indices
folds <- Folds(y, k=10)
# glmt.net
results <- lapply(folds, function(fold, y, x, parallel=T) {
return(glmnet.wrapper(y=y[-fold],
x=x[-fold, , drop=F],
newx=x[fold,  , drop=F],
newy=y[fold]))
}, y=y, x=x)
# plot
for (i in 1:length(folds)) {
plot(results[[i]]$best.model, main=names(results[[i]]$pred))
}
results[[1]]
# libraries
library(MASS)  # Package needed to generate correlated precictors
library(glmnet)  # Package to fit ridge/lasso/elastic net models
source("elastic.net.R")
# Generate data
set.seed(19874)
n <- 10    # Number of observations
p <- 500     # Number of predictors included in model
real_p <- 150  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
nrow(x)
# add colnames
rownames(x) <- sprintf("sample%s", seq(1:nrow(x)))
# vector names
y <- setNames(y, sprintf("sample%s", seq(1:length(y))))
# --------------------------------------------
# Remove NA values
y   <- y[!is.na(y)]
x   <- na.omit(x)
ol  <- intersect(rownames(x), names(y))
y   <- y[ol]
x   <- x[ol, , drop=F]
# create a list of length k, containing the test-set indices
folds <- Folds(y, k=10)
# glmt.net
results <- lapply(folds, function(fold, y, x, parallel=F) {
return(glmnet.wrapper(y=y[-fold],
x=x[-fold, , drop=F],
newx=x[fold,  , drop=F],
newy=y[fold]))
}, y=y, x=x)
# plot
for (i in 1:length(folds)) {
plot(results[[i]]$best.model, main=names(results[[i]]$pred))
}
#
results[[1]]
results[[1]]
folds[1]
Folds(y, k=10)
fit.elnet.cv <- cv.glmnet(x[-fold[1], , drop=F], y[-fold[1]], type.measure="mse", alpha=.5,
family="gaussian")
Folds <- function(y, k=10) {
n <- length(y)
if (n == 0)
stop('response length is zero')
uniqY <- unique(y)
if (!is.factor(y) && length(y) / length(uniqY) >= k) {
# Intepret the integer-valued y as class labels. Stratify if the number of class labels is <= 5.
y <- factor(y)
} else if (is.numeric(y)) {
# 5-stratum Stratified sampling
if (n >= 5 * k) {
breaks <- unique(quantile(y, probs=seq(0, 1, length.out=5)))
y <- as.integer(cut(y, breaks, include.lowest=TRUE))
} else
y <- rep(1, length(y))
}
sampList <- tapply(seq_along(y), y, sFolds, k=k, simplify=FALSE)
list0 <- list()
length(list0) <- k
samp <- Reduce(function(list1, list2) {
mapply(c, list1, list2, SIMPLIFY=FALSE)
}, sampList, list0)
return(samp)
}
sFolds <- function(yy, k=10) {
if (length(yy) > 1)
allSamp <- sample(yy)
else
allSamp <- yy
n <- length(yy)
nEach <- n %/% k
samp <- list()
length(samp) <- k
for (i in seq_along(samp)) {
if (nEach > 0)
samp[[i]] <- allSamp[1:nEach + (i - 1) * nEach]
else
samp[[i]] <- numeric(0)
}
restSamp <- allSamp[seq(nEach * k + 1, length(allSamp), length.out=length(allSamp) - nEach * k)]
restInd <- sample(k, length(restSamp))
for (i in seq_along(restInd)) {
sampInd <- restInd[i]
samp[[sampInd]] <- c(samp[[sampInd]], restSamp[i])
}
return(samp)
}
# libraries
library(MASS)  # Package needed to generate correlated precictors
library(glmnet)  # Package to fit ridge/lasso/elastic net models
source("elastic.net.R")
# Generate data
set.seed(19874)
n <- 10    # Number of observations
p <- 500     # Number of predictors included in model
real_p <- 150  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
nrow(x)
# add colnames
rownames(x) <- sprintf("sample%s", seq(1:nrow(x)))
# vector names
y <- setNames(y, sprintf("sample%s", seq(1:length(y))))
# --------------------------------------------
# Remove NA values
y   <- y[!is.na(y)]
x   <- na.omit(x)
ol  <- intersect(rownames(x), names(y))
y   <- y[ol]
x   <- x[ol, , drop=F]
# create a list of length k, containing the test-set indices
folds <- Folds(y, k=10)
# glmt.net
results <- lapply(folds, function(fold, y, x, parallel=F) {
return(glmnet.wrapper(y=y[-fold],
x=x[-fold, , drop=F],
newx=x[fold,  , drop=F],
newy=y[fold]))
}, y=y, x=x)
# plot
for (i in 1:length(folds)) {
plot(results[[i]]$best.model, main=names(results[[i]]$pred))
}
a <- seq(0, 1, .1)
mse <- c()
for (i in 1:10) {
m <- results[[i]]$mse
mse <- c(mse, m)
}
ggplot(data = NULL, aes(a, mse) ) +
geom_point() +
geom_line(col="grey") +
labs(subtitle="Alpha levels ~ Mean-Squared Error",
x="alpha",
y="Mean-Squared Error",
title="MSE on test set")
# libraries
library(MASS)  # Package needed to generate correlated precictors
library(glmnet)  # Package to fit ridge/lasso/elastic net models
library(ggplot2)
source("elastic.net.R")
# Generate data
set.seed(19874)
n <- 10    # Number of observations
p <- 500     # Number of predictors included in model
real_p <- 150  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
nrow(x)
# add colnames
rownames(x) <- sprintf("sample%s", seq(1:nrow(x)))
# vector names
y <- setNames(y, sprintf("sample%s", seq(1:length(y))))
# --------------------------------------------
# Remove NA values
y   <- y[!is.na(y)]
x   <- na.omit(x)
ol  <- intersect(rownames(x), names(y))
y   <- y[ol]
x   <- x[ol, , drop=F]
# create a list of length k, containing the test-set indices
folds <- Folds(y, k=10)
Folds <- function(y, k=10) {
n <- length(y)
if (n == 0)
stop('response length is zero')
uniqY <- unique(y)
if (!is.factor(y) && length(y) / length(uniqY) >= k) {
# Intepret the integer-valued y as class labels. Stratify if the number of class labels is <= 5.
y <- factor(y)
} else if (is.numeric(y)) {
# 5-stratum Stratified sampling
if (n >= 5 * k) {
breaks <- unique(quantile(y, probs=seq(0, 1, length.out=5)))
y <- as.integer(cut(y, breaks, include.lowest=TRUE))
} else
y <- rep(1, length(y))
}
sampList <- tapply(seq_along(y), y, sFolds, k=k, simplify=FALSE)
list0 <- list()
length(list0) <- k
samp <- Reduce(function(list1, list2) {
mapply(c, list1, list2, SIMPLIFY=FALSE)
}, sampList, list0)
return(samp)
}
sFolds <- function(yy, k=10) {
if (length(yy) > 1)
allSamp <- sample(yy)
else
allSamp <- yy
n <- length(yy)
nEach <- n %/% k
samp <- list()
length(samp) <- k
for (i in seq_along(samp)) {
if (nEach > 0)
samp[[i]] <- allSamp[1:nEach + (i - 1) * nEach]
else
samp[[i]] <- numeric(0)
}
restSamp <- allSamp[seq(nEach * k + 1, length(allSamp), length.out=length(allSamp) - nEach * k)]
restInd <- sample(k, length(restSamp))
for (i in seq_along(restInd)) {
sampInd <- restInd[i]
samp[[sampInd]] <- c(samp[[sampInd]], restSamp[i])
}
return(samp)
}
# libraries
library(MASS)  # Package needed to generate correlated precictors
library(glmnet)  # Package to fit ridge/lasso/elastic net models
library(ggplot2)
source("elastic.net.R")
# Generate data
set.seed(19874)
n <- 10    # Number of observations
p <- 500     # Number of predictors included in model
real_p <- 150  # Number of true predictors
x <- matrix(rnorm(n*p), nrow=n, ncol=p)
y <- apply(x[,1:real_p], 1, sum) + rnorm(n)
nrow(x)
# add colnames
rownames(x) <- sprintf("sample%s", seq(1:nrow(x)))
# vector names
y <- setNames(y, sprintf("sample%s", seq(1:length(y))))
# --------------------------------------------
# Remove NA values
y   <- y[!is.na(y)]
x   <- na.omit(x)
ol  <- intersect(rownames(x), names(y))
y   <- y[ol]
x   <- x[ol, , drop=F]
# create a list of length k, containing the test-set indices
folds <- Folds(y, k=10)
# glmt.net
results <- lapply(folds, function(fold, y, x, parallel=F) {
return(glmnet.wrapper(y=y[-fold],
x=x[-fold, , drop=F],
newx=x[fold,  , drop=F],
newy=y[fold]))
}, y=y, x=x)
# plot
for (i in 1:length(folds)) {
plot(results[[i]]$best.model, main=names(results[[i]]$pred))
}
results[[1]]$mse
# calculate Mean-Squared error of model
a <- seq(0, 1, .1)
mse <- c()
for (i in 1:10) {
m <- results[[i]]$mse
mse <- c(mse, m)
}
mse
a
length(a)
a <- seq(0.1, 1, .1)
length(a)
ggplot(data = NULL, aes(a, mse) ) +
geom_point() +
geom_line(col="grey") +
labs(subtitle="Alpha levels ~ Mean-Squared Error",
x="alpha",
y="Mean-Squared Error",
title="MSE on test set")
